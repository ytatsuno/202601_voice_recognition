{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8562255",
   "metadata": {},
   "source": [
    "# Keyword Spotting: 「asial」 vs 指示コマンド vs unknown / background_noise（TFJS出力）\n",
    "\n",
    "このノートブックは、DigiKey のチュートリアルが説明している **Speech Commands + _background_noise_ + 自作キーワード** の考え方（1秒クリップ、16kHz、ノイズ混ぜ、MFCC→CNN）を Colab 上で再現し、最後に **TensorFlow.js (LayersModel) の `model.json`** を出力します。  \n",
    "参考：DigiKey 記事（Speech Commands の取得、_background_noise_ の分離、16kHz/1秒、MFCC+CNN） citeturn12view0\n",
    "\n",
    "---\n",
    "\n",
    "## このノートでやること（ざっくり）\n",
    "1. Speech Commands をダウンロードして、指示コマンドと unknown を作る  \n",
    "2. _background_noise_ から 1秒ノイズ片を作り、さらに学習時にランダムにミックス  \n",
    "3. `asial` の自作音声（あなたの録音）を読み込み  \n",
    "4. WAV を **16kHz / mono / 1.0秒 / float32** に統一（重要）  \n",
    "5. MFCC を作って CNN を学習  \n",
    "6. TFJS 向けに `model.json` を出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ada7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0) インストール（Colabの依存衝突を避ける版）\n",
    "# ポイント:\n",
    "# - Colab既存の bigquery/xarray 等が packaging>=24 を要求する一方、tensorflowjs が packaging 23.x を要求して衝突しがちです。\n",
    "# - ここでは tensorflowjs を --no-deps で入れて変換ツールだけ使います（変換自体は通常これで動きます）。\n",
    "# - DEMAND/LibriSpeech 取り込みを使いたい場合は datasets を追加で入れてください（ただし fsspec の衝突に注意）。\n",
    "\n",
    "!pip -q install -U pip\n",
    "\n",
    "# 学習に必要なもの\n",
    "!pip -q install   \"tensorflow==2.19.0\"   \"tensorflow-decision-forests==1.12.0\"   \"librosa==0.10.1\" \"soundfile==0.12.1\"   \"tqdm>=4.67\" \"scikit-learn>=1.6\"\n",
    "\n",
    "# TFJS 変換ツール（依存解決はしない）\n",
    "!pip -q install \"tensorflowjs==4.22.0\" --no-deps\n",
    "\n",
    "import sys, tensorflow as tf\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "\n",
    "import tensorflowjs as tfjs\n",
    "print(\"TensorFlow.js (python):\", tfjs.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8873b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title （変換でKeras互換性エラーが出る場合のみ）Keras 2互換に切り替え\n",
    "# TF 2.16+ は既定で Keras 3 になるため、必要なら tf-keras + TF_USE_LEGACY_KERAS で Keras 2 を使えます citeturn5search3\n",
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"  # TensorFlow import より前に設定する必要があります citeturn5search3\n",
    "!pip -q install \"tf-keras~=2.19\"\n",
    "print(\"Set TF_USE_LEGACY_KERAS=1 and installed tf-keras.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5db390e",
   "metadata": {},
   "source": [
    "## 1) 設定（ラベルなど）\n",
    "- 指示コマンド（例）：`up, down, left, right, go, stop`  \n",
    "- 自作キーワード：`asial`  \n",
    "- 追加クラス：`unknown`, `background_noise`\n",
    "\n",
    "※ Speech Commands recognizer の既定語彙にも `unknown` と `background_noise` が含まれます citeturn7search5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc7762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, glob, shutil, math\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# ---- ここを必要に応じて変更 ----\n",
    "COMMAND_WORDS = [\"up\", \"down\", \"left\", \"right\", \"go\", \"stop\"]   # 指示コマンド\n",
    "CUSTOM_WORD   = \"asial\"                      # 自作キーワード（フォルダ名）\n",
    "LABELS = COMMAND_WORDS + [CUSTOM_WORD, \"unknown\", \"background_noise\"]\n",
    "\n",
    "SR = 16000\n",
    "CLIP_SECONDS = 1.0\n",
    "CLIP_SAMPLES = int(SR * CLIP_SECONDS)\n",
    "\n",
    "# データ配置\n",
    "WORK = Path(\"/content/kws\")\n",
    "RAW  = WORK / \"raw\"      # 収集した（まだバラバラな）wav\n",
    "STD  = WORK / \"std\"      # 16kHz/mono/1s に統一した wav\n",
    "NOISE_POOL = WORK / \"noise_pool\"  # ミックス用ノイズ片\n",
    "for p in [RAW, STD, NOISE_POOL]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"LABELS:\", LABELS)\n",
    "print(\"WORK:\", WORK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb42d16",
   "metadata": {},
   "source": [
    "## 2) Speech Commands を取得して、指示コマンド / unknown / background_noise を作る\n",
    "\n",
    "DigiKey 記事でも、Speech Commands をダウンロードして `_background_noise_` を分離して使う手順が説明されています citeturn12view0  \n",
    "Speech Commands は CC BY 4.0 で公開されています（引用元例） citeturn0search5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc068bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2-1) Speech Commands v0.02 をダウンロードして展開\n",
    "import tarfile, urllib.request\n",
    "\n",
    "speech_tar = WORK / \"speech_commands_v0.02.tar.gz\"\n",
    "speech_root = WORK / \"speech_commands_v0.02\"\n",
    "\n",
    "if not speech_root.exists():\n",
    "    if not speech_tar.exists():\n",
    "        url = \"https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz\"\n",
    "        print(\"Downloading:\", url)\n",
    "        urllib.request.urlretrieve(url, speech_tar)\n",
    "    print(\"Extracting...\")\n",
    "    speech_root.mkdir(parents=True, exist_ok=True)\n",
    "    with tarfile.open(speech_tar, \"r:gz\") as tar:\n",
    "        tar.extractall(path=speech_root)\n",
    "print(\"speech_root:\", speech_root)\n",
    "print(\"Example dirs:\", sorted([p.name for p in speech_root.iterdir() if p.is_dir()])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91cd6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2-2) 指示コマンド WAV を RAW にコピー\n",
    "def copy_some(src_dir: Path, dst_dir: Path, max_files=None):\n",
    "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "    files = sorted(src_dir.glob(\"*.wav\"))\n",
    "    if max_files is not None:\n",
    "        random.shuffle(files)\n",
    "        files = files[:max_files]\n",
    "    for f in files:\n",
    "        shutil.copy2(f, dst_dir / f.name)\n",
    "    return len(files)\n",
    "\n",
    "# 指示コマンド：各フォルダを丸ごとコピー（必要なら max_files で制限）\n",
    "for w in COMMAND_WORDS:\n",
    "    n = copy_some(speech_root / w, RAW / w, max_files=None)\n",
    "    print(w, n)\n",
    "\n",
    "# background_noise の元（長い wav）\n",
    "bg_src = speech_root / \"_background_noise_\"\n",
    "print(\"background_noise sources:\", len(list(bg_src.glob(\"*.wav\"))), bg_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79044a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2-3) unknown を作る：Speech Commands の「それ以外の単語」からランダム抽出\n",
    "# unknown の比率は調整ポイント。とりあえず各コマンド合計と同程度に作る。\n",
    "target_unknown = sum(len(list((RAW / w).glob(\"*.wav\"))) for w in COMMAND_WORDS)\n",
    "\n",
    "exclude = set(COMMAND_WORDS + [CUSTOM_WORD, \"_background_noise_\", \"unknown\", \"background_noise\"])\n",
    "candidate_dirs = [p for p in speech_root.iterdir() if p.is_dir() and p.name not in exclude and not p.name.startswith(\".\")]\n",
    "\n",
    "cand_files = []\n",
    "for d in candidate_dirs:\n",
    "    cand_files.extend(list(d.glob(\"*.wav\")))\n",
    "random.shuffle(cand_files)\n",
    "cand_files = cand_files[:target_unknown]\n",
    "\n",
    "dst = RAW / \"unknown\"\n",
    "dst.mkdir(parents=True, exist_ok=True)\n",
    "for f in cand_files:\n",
    "    shutil.copy2(f, dst / f\"{f.parent.name}_{f.name}\")\n",
    "\n",
    "print(\"unknown:\", len(list(dst.glob(\"*.wav\"))), \"target:\", target_unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad226d2",
   "metadata": {},
   "source": [
    "## 3) background_noise クラス用の 1秒ノイズ片を作る（+ ノイズプール）\n",
    "\n",
    "DigiKey 記事でも、`_background_noise_` の長尺 WAV を使ってノイズを混ぜる（データ拡張）流れが説明されています citeturn12view0  \n",
    "ここでは\n",
    "- **background_noise（分類クラス）用**：純ノイズ 1秒片を大量に作る  \n",
    "- **noise_pool（ミックス用）**：同じものを流用  \n",
    "を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98bcdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3) _background_noise_ から 1秒ノイズ片を生成\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "\n",
    "def ensure_mono(x):\n",
    "    if x.ndim == 1:\n",
    "        return x\n",
    "    return np.mean(x, axis=1)\n",
    "\n",
    "def pad_or_trim(x, n):\n",
    "    if len(x) < n:\n",
    "        return np.pad(x, (0, n - len(x)))\n",
    "    return x[:n]\n",
    "\n",
    "def slice_noise_file(wav_path: Path, out_dir: Path, n_clips: int, prefix: str):\n",
    "    y, sr = librosa.load(str(wav_path), sr=SR, mono=True)\n",
    "    if len(y) < CLIP_SAMPLES:\n",
    "        return 0\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    count = 0\n",
    "    max_start = len(y) - CLIP_SAMPLES\n",
    "    for i in range(n_clips):\n",
    "        start = random.randint(0, max_start)\n",
    "        clip = y[start:start+CLIP_SAMPLES]\n",
    "        out = out_dir / f\"{prefix}_{wav_path.stem}_{i:04d}.wav\"\n",
    "        sf.write(out, clip.astype(np.float32), SR, subtype=\"PCM_16\")\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "bg_dst = RAW / \"background_noise\"\n",
    "bg_dst.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 各ノイズファイルから生成する数（必要に応じて増減）\n",
    "CLIPS_PER_BG_FILE = 400\n",
    "\n",
    "total = 0\n",
    "for f in tqdm(sorted(bg_src.glob(\"*.wav\"))):\n",
    "    total += slice_noise_file(f, bg_dst, CLIPS_PER_BG_FILE, prefix=\"bg\")\n",
    "print(\"background_noise clips:\", total)\n",
    "\n",
    "# noise_pool にもコピー（ミックス用に使う）\n",
    "NOISE_POOL.mkdir(parents=True, exist_ok=True)\n",
    "for f in bg_dst.glob(\"*.wav\"):\n",
    "    shutil.copy2(f, NOISE_POOL / f.name)\n",
    "\n",
    "print(\"noise_pool clips:\", len(list(NOISE_POOL.glob('*.wav'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4de10d",
   "metadata": {},
   "source": [
    "## 4) 追加のオープンソースデータセット（任意）\n",
    "\n",
    "「unknown」や「background_noise」をより多様にするなら、Speech Commands 以外も混ぜるのが有効です。\n",
    "\n",
    "### 4-A) DEMAND（環境ノイズ）\n",
    "- 実環境のノイズ録音が入っているデータセット（Hugging Face でも入手例あり） citeturn1search0turn1search8  \n",
    "- ライセンスは CC BY-SA 3.0 として提供されている配布元があります citeturn1search16\n",
    "\n",
    "### 4-B) MUSAN（noise/music/speech）\n",
    "- ノイズ・音楽・音声を含むコーパス。論文では「柔軟な Creative Commons ライセンス」で公開と説明されています citeturn0search10turn0search2\n",
    "\n",
    "### 4-C) LibriSpeech（unknown の“人の声”を増やす）\n",
    "- 大規模な朗読音声。CC BY 4.0 citeturn0search3\n",
    "\n",
    "### 4-D) Mozilla Common Voice（unknown を多言語で増やす）\n",
    "- CC0 でデータセット配布（ただし配布経路の注意が書かれています） citeturn1search7turn1search3\n",
    "\n",
    "このノートでは **軽量に試せる DEMAND と LibriSpeech の “小さめ” 手順** を用意します（実行は任意）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec8329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4-A) （任意）DEMAND ノイズを noise_pool に追加（短いデモ）\n",
    "# 実行すると、Hugging Face から DEMAND の音声を取得して 1秒ノイズ片を追加します。\n",
    "# 環境ノイズの多様性が上がります。\n",
    "\n",
    "USE_DEMAND = False  #@param {type:\"boolean\"}\n",
    "\n",
    "if USE_DEMAND:\n",
    "    from datasets import load_dataset\n",
    "    import numpy as np\n",
    "    import soundfile as sf\n",
    "\n",
    "    ds = load_dataset(\"voice-biomarkers/DEMAND-acoustic-noise\", split=\"train\")\n",
    "    out_dir = NOISE_POOL / \"demand\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    clips_per_item = 40\n",
    "    total = 0\n",
    "    for i in range(min(len(ds), 15)):\n",
    "        arr = ds[i][\"audio\"][\"array\"]\n",
    "        sr0 = ds[i][\"audio\"][\"sampling_rate\"]\n",
    "        y = librosa.resample(arr.astype(np.float32), orig_sr=sr0, target_sr=SR)\n",
    "        if len(y) < CLIP_SAMPLES:\n",
    "            continue\n",
    "        max_start = len(y) - CLIP_SAMPLES\n",
    "        for j in range(clips_per_item):\n",
    "            start = random.randint(0, max_start)\n",
    "            clip = y[start:start+CLIP_SAMPLES]\n",
    "            sf.write(out_dir / f\"demand_{i:02d}_{j:03d}.wav\", clip.astype(np.float32), SR, subtype=\"PCM_16\")\n",
    "            total += 1\n",
    "    print(\"Added DEMAND noise clips:\", total)\n",
    "    print(\"noise_pool total:\", len(list(NOISE_POOL.rglob('*.wav'))))\n",
    "else:\n",
    "    print(\"Skipped DEMAND.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4942316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4-C) （任意）LibriSpeech から unknown（人の声）を追加（dev-clean の一部だけ）\n",
    "USE_LIBRISPEECH = False  #@param {type:\"boolean\"}\n",
    "\n",
    "if USE_LIBRISPEECH:\n",
    "    import tarfile, urllib.request\n",
    "    lib_root = WORK / \"librispeech_dev_clean\"\n",
    "    tgz = WORK / \"dev-clean.tar.gz\"\n",
    "    if not lib_root.exists():\n",
    "        url = \"https://www.openslr.org/resources/12/dev-clean.tar.gz\"\n",
    "        print(\"Downloading:\", url)\n",
    "        urllib.request.urlretrieve(url, tgz)\n",
    "        print(\"Extracting...\")\n",
    "        lib_root.mkdir(parents=True, exist_ok=True)\n",
    "        with tarfile.open(tgz, \"r:gz\") as tar:\n",
    "            tar.extractall(path=lib_root)\n",
    "\n",
    "    # flac -> wav 変換せず、librosa で直接読み込んで 1秒切り出し（軽量）\n",
    "    flacs = list(lib_root.rglob(\"*.flac\"))\n",
    "    random.shuffle(flacs)\n",
    "    flacs = flacs[:60]  # 取りすぎ防止\n",
    "\n",
    "    out_dir = RAW / \"unknown_librispeech\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    total = 0\n",
    "    for f in tqdm(flacs):\n",
    "        y, sr0 = librosa.load(str(f), sr=SR, mono=True)\n",
    "        if len(y) < CLIP_SAMPLES:\n",
    "            continue\n",
    "        max_start = len(y) - CLIP_SAMPLES\n",
    "        # 1ファイルから数枚だけ\n",
    "        for j in range(3):\n",
    "            start = random.randint(0, max_start)\n",
    "            clip = y[start:start+CLIP_SAMPLES]\n",
    "            sf.write(out_dir / f\"ls_{f.stem}_{j}.wav\", clip.astype(np.float32), SR, subtype=\"PCM_16\")\n",
    "            total += 1\n",
    "\n",
    "    print(\"Added LibriSpeech unknown clips:\", total)\n",
    "\n",
    "    # unknown にマージ\n",
    "    unk_dir = RAW / \"unknown\"\n",
    "    for f in out_dir.glob(\"*.wav\"):\n",
    "        shutil.copy2(f, unk_dir / f.name)\n",
    "\n",
    "    print(\"unknown total:\", len(list(unk_dir.glob('*.wav'))))\n",
    "else:\n",
    "    print(\"Skipped LibriSpeech.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab358a6",
   "metadata": {},
   "source": [
    "## 5) `asial` の自作データを用意（アップロード or Drive）\n",
    "以下どちらかで `asial` の WAV を `RAW/asial/` に置きます。\n",
    "\n",
    "- A) Colab に ZIP をアップロード（中に wav が入っていればOK）\n",
    "- B) Google Drive からコピー\n",
    "\n",
    "※ DigiKey 記事では、カスタムキーワードは「最低50サンプル、1秒、16kHz、WAV」を推奨しています citeturn12view0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5995fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5-A) Colab にZIPをアップロード（任意）\n",
    "from google.colab import files\n",
    "\n",
    "UPLOAD_ZIP = False  #@param {type:\"boolean\"}\n",
    "if UPLOAD_ZIP:\n",
    "    uploaded = files.upload()\n",
    "    # 1つ目のzipを使う想定\n",
    "    zip_path = next(iter(uploaded.keys()))\n",
    "    print(\"Uploaded:\", zip_path)\n",
    "\n",
    "    import zipfile\n",
    "    asial_dir = RAW / CUSTOM_WORD\n",
    "    asial_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        z.extractall(asial_dir)\n",
    "\n",
    "    # zip内に階層がある場合を吸収（wavを集約）\n",
    "    wavs = list(asial_dir.rglob(\"*.wav\"))\n",
    "    flat_dir = asial_dir\n",
    "    for w in wavs:\n",
    "        if w.parent != flat_dir:\n",
    "            shutil.copy2(w, flat_dir / w.name)\n",
    "\n",
    "    print(\"asial wav count:\", len(list((RAW / CUSTOM_WORD).glob('*.wav'))))\n",
    "else:\n",
    "    print(\"Skip upload.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcefc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5-B) Google Drive からコピー（任意）\n",
    "USE_DRIVE = False  #@param {type:\"boolean\"}\n",
    "DRIVE_ASIAL_DIR = \"/content/drive/MyDrive/asial_wavs\"  #@param {type:\"string\"}\n",
    "\n",
    "if USE_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "    src = Path(DRIVE_ASIAL_DIR)\n",
    "    assert src.exists(), f\"not found: {src}\"\n",
    "    dst = RAW / CUSTOM_WORD\n",
    "    dst.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for f in src.glob(\"*.wav\"):\n",
    "        shutil.copy2(f, dst / f.name)\n",
    "    print(\"asial wav count:\", len(list(dst.glob('*.wav'))))\n",
    "else:\n",
    "    print(\"Skip drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5468d9f",
   "metadata": {},
   "source": [
    "## 6) WAV を統一（16kHz / mono / 1.0秒 / PCM16（16-bit PCM））\n",
    "\n",
    "ここが「一番ハマる」ポイントです（形式が揃ってないと学習が壊れやすい）。  \n",
    "DigiKey 記事でも 16kHz・1秒・WAV を強調しています citeturn12view0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cf71a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6) 全ラベルの wav を標準化して STD に出力\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "\n",
    "def standardize_wav(in_path: Path, out_path: Path):\n",
    "    y, sr0 = librosa.load(str(in_path), sr=SR, mono=True)  # resample + mono\n",
    "    y = pad_or_trim(y.astype(np.float32), CLIP_SAMPLES)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    sf.write(str(out_path), y, SR, subtype=\"PCM_16\")\n",
    "\n",
    "# まず STD を空にする\n",
    "if STD.exists():\n",
    "    shutil.rmtree(STD)\n",
    "STD.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# RAW の各ラベルから STD へ\n",
    "for label in LABELS:\n",
    "    src_dir = RAW / label\n",
    "    if not src_dir.exists():\n",
    "        print(\"WARN: missing label dir:\", src_dir)\n",
    "        continue\n",
    "    files = sorted(src_dir.glob(\"*.wav\"))\n",
    "    for f in tqdm(files, desc=f\"std:{label}\"):\n",
    "        out = STD / label / f.name\n",
    "        standardize_wav(f, out)\n",
    "\n",
    "for label in LABELS:\n",
    "    print(label, len(list((STD/label).glob(\"*.wav\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf65ffde",
   "metadata": {},
   "source": [
    "## 7) 学習用 Dataset（tf.data）を作る\n",
    "- 学習時に **noise_pool からランダムにノイズを混ぜる**（SNRランダム）  \n",
    "- 特徴量は **MFCC**（DigiKey 記事が採用している特徴量） citeturn12view0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d59e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "label_to_id = {l:i for i,l in enumerate(LABELS)}\n",
    "id_to_label = {i:l for l,i in label_to_id.items()}\n",
    "\n",
    "# noise pool files\n",
    "noise_files = [str(p) for p in NOISE_POOL.rglob(\"*.wav\")]\n",
    "print(\"noise_files:\", len(noise_files))\n",
    "\n",
    "def load_wav(path):\n",
    "    audio = tf.io.read_file(path)\n",
    "    wav, sr = tf.audio.decode_wav(audio, desired_channels=1)\n",
    "    wav = tf.squeeze(wav, axis=-1)  # [samples]\n",
    "    wav = wav[:CLIP_SAMPLES]\n",
    "    wav = tf.cond(tf.shape(wav)[0] < CLIP_SAMPLES,\n",
    "                  lambda: tf.pad(wav, [[0, CLIP_SAMPLES - tf.shape(wav)[0]]]),\n",
    "                  lambda: wav)\n",
    "    return wav\n",
    "\n",
    "def random_mix_noise(wav):\n",
    "    if len(noise_files) == 0:\n",
    "        return wav\n",
    "    nf = tf.random.uniform([], 0, len(noise_files), dtype=tf.int32)\n",
    "    noise = load_wav(tf.constant(noise_files)[nf])\n",
    "\n",
    "    # SNR: 0〜20dB（調整可）\n",
    "    snr_db = tf.random.uniform([], 0.0, 20.0)\n",
    "    wav_rms = tf.sqrt(tf.reduce_mean(tf.square(wav)) + 1e-9)\n",
    "    noi_rms = tf.sqrt(tf.reduce_mean(tf.square(noise)) + 1e-9)\n",
    "\n",
    "    snr = tf.pow(10.0, snr_db / 20.0)\n",
    "    scale = wav_rms / (snr * noi_rms + 1e-9)\n",
    "    mixed = wav + noise * scale\n",
    "\n",
    "    # クリップ防止\n",
    "    mixed = tf.clip_by_value(mixed, -1.0, 1.0)\n",
    "    return mixed\n",
    "\n",
    "def wav_to_mfcc(wav):\n",
    "    # STFT\n",
    "    frame_length = 640   # 40ms @16k\n",
    "    frame_step   = 320   # 20ms\n",
    "    fft_length   = 1024\n",
    "    stft = tf.signal.stft(wav, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length)\n",
    "    spectrogram = tf.abs(stft)\n",
    "\n",
    "    # mel\n",
    "    num_spectrogram_bins = spectrogram.shape[-1]\n",
    "    num_mel_bins = 40\n",
    "    lower_edge_hertz, upper_edge_hertz = 20.0, SR/2\n",
    "    mel_w = tf.signal.linear_to_mel_weight_matrix(\n",
    "        num_mel_bins, num_spectrogram_bins, SR, lower_edge_hertz, upper_edge_hertz)\n",
    "    mel = tf.tensordot(spectrogram, mel_w, 1)\n",
    "    mel.set_shape(spectrogram.shape[:-1].concatenate(mel_w.shape[-1:]))\n",
    "\n",
    "    log_mel = tf.math.log(mel + 1e-6)\n",
    "    mfcc = tf.signal.mfccs_from_log_mel_spectrograms(log_mel)[..., :13]  # 13次元\n",
    "    # CNN用に [T, F, 1]\n",
    "    mfcc = mfcc[..., tf.newaxis]\n",
    "    return mfcc\n",
    "\n",
    "def make_file_label_lists():\n",
    "    paths = []\n",
    "    labels = []\n",
    "    for label in LABELS:\n",
    "        for f in (STD / label).glob(\"*.wav\"):\n",
    "            paths.append(str(f))\n",
    "            labels.append(label_to_id[label])\n",
    "    # shuffle\n",
    "    idx = list(range(len(paths)))\n",
    "    random.shuffle(idx)\n",
    "    paths = [paths[i] for i in idx]\n",
    "    labels = [labels[i] for i in idx]\n",
    "    return paths, labels\n",
    "\n",
    "paths, labels = make_file_label_lists()\n",
    "print(\"total files:\", len(paths))\n",
    "\n",
    "# train/val split\n",
    "split = int(0.85 * len(paths))\n",
    "train_paths, val_paths = paths[:split], paths[split:]\n",
    "train_labels, val_labels = labels[:split], labels[split:]\n",
    "\n",
    "def build_ds(paths, labels, training: bool):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    if training:\n",
    "        ds = ds.shuffle(2048, seed=SEED, reshuffle_each_iteration=True)\n",
    "    def _map(p, y):\n",
    "        wav = load_wav(p)\n",
    "        if training:\n",
    "            wav = random_mix_noise(wav)\n",
    "        x = wav_to_mfcc(wav)\n",
    "        return x, tf.one_hot(y, depth=len(LABELS))\n",
    "    ds = ds.map(_map, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(64).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = build_ds(train_paths, train_labels, True)\n",
    "val_ds   = build_ds(val_paths, val_labels, False)\n",
    "\n",
    "# shape check\n",
    "for x,y in train_ds.take(1):\n",
    "    print(\"x:\", x.shape, \"y:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7660451f",
   "metadata": {},
   "source": [
    "## 8) モデル（CNN）を学習\n",
    "\n",
    "DigiKey 記事では Edge Impulse の “Audio (MFCC) + Neural Network (Keras)” を使ってキーワード分類を行っています citeturn12view0  \n",
    "ここでは同じ考え方で、MFCC を画像っぽく扱う CNN を学習します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c64fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 入力形状を ds から取る\n",
    "for xb, yb in train_ds.take(1):\n",
    "    input_shape = xb.shape[1:]\n",
    "print(\"input_shape:\", input_shape)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Conv2D(16, (3,3), activation=\"relu\", padding=\"same\"),\n",
    "    layers.MaxPool2D((2,2)),\n",
    "    layers.Conv2D(32, (3,3), activation=\"relu\", padding=\"same\"),\n",
    "    layers.MaxPool2D((2,2)),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Conv2D(64, (3,3), activation=\"relu\", padding=\"same\"),\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(len(LABELS), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=6, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=40,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b858a27c",
   "metadata": {},
   "source": [
    "## 9) 評価（混同行列）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef8054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# 予測\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for xb, yb in val_ds:\n",
    "    yp = model.predict(xb, verbose=0)\n",
    "    y_true.extend(np.argmax(yb.numpy(), axis=1))\n",
    "    y_pred.extend(np.argmax(yp, axis=1))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "print(cm)\n",
    "print()\n",
    "print(classification_report(y_true, y_pred, target_names=LABELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f119c6ba",
   "metadata": {},
   "source": [
    "## 10) TFJS（model.json）に変換して出力\n",
    "\n",
    "このノートは Keras モデルを **TensorFlow.js LayersModel** に変換して `model.json` を生成します。  \n",
    "TensorFlow.js の Speech Commands 系のカスタム学習も「Keras→TFJS 変換」を前提にしています citeturn2search2turn2search1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8234e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflowjs as tfjs\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "# --- TFJS出力先（ブラウザ側のHTMLと合わせる） ---\n",
    "export_dir = WORK / \"tfjs_model_tfonly\"\n",
    "if export_dir.exists():\n",
    "    shutil.rmtree(export_dir)\n",
    "export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- モデル保存 ---\n",
    "tfjs.converters.save_keras_model(model, str(export_dir))\n",
    "print(\"Saved TFJS model to:\", export_dir)\n",
    "print(\"Files:\", [p.name for p in export_dir.iterdir()])\n",
    "\n",
    "# --- 前処理パラメータ（JS側と一致させる） ---\n",
    "FRAME_LENGTH = 640  # 40ms @ 16k\n",
    "FRAME_STEP   = 320  # 20ms\n",
    "FFT_LENGTH   = 1024\n",
    "\n",
    "NUM_MEL_BINS = 40\n",
    "LOWER_EDGE_HZ, UPPER_EDGE_HZ = 20.0, SR/2\n",
    "\n",
    "NUM_MFCC = 13\n",
    "NUM_SPECTROGRAM_BINS = FFT_LENGTH // 2 + 1\n",
    "\n",
    "# ラベル + 前処理情報（JS側で読めるように）\n",
    "with open(export_dir / \"labels.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"labels\": LABELS,\n",
    "        \"sample_rate\": int(SR),\n",
    "        \"clip_samples\": int(CLIP_SAMPLES),\n",
    "        \"frame_length\": int(FRAME_LENGTH),\n",
    "        \"frame_step\": int(FRAME_STEP),\n",
    "        \"fft_length\": int(FFT_LENGTH),\n",
    "        \"num_mel_bins\": int(NUM_MEL_BINS),\n",
    "        \"lower_edge_hz\": float(LOWER_EDGE_HZ),\n",
    "        \"upper_edge_hz\": float(UPPER_EDGE_HZ),\n",
    "        \"num_mfcc\": int(NUM_MFCC),\n",
    "        \"num_spectrogram_bins\": int(NUM_SPECTROGRAM_BINS)\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# --- 方式B: 学習側(TF)と同じ行列を吐き出してJSで使う ---\n",
    "# 1) Mel重み行列: shape [num_spectrogram_bins, num_mel_bins]\n",
    "mel_w = tf.signal.linear_to_mel_weight_matrix(\n",
    "    NUM_MEL_BINS, NUM_SPECTROGRAM_BINS, SR, LOWER_EDGE_HZ, UPPER_EDGE_HZ\n",
    ")\n",
    "mel_w_np = mel_w.numpy().astype(\"float32\")\n",
    "\n",
    "with open(export_dir / \"mel_weight_matrix.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"shape\": list(mel_w_np.shape),\n",
    "        \"data\": mel_w_np.flatten().tolist()\n",
    "    }, f)\n",
    "\n",
    "# 2) DCT行列: tf.signal.mfccs_from_log_mel_spectrograms 相当\n",
    "#    mfccs_from_log_mel_spectrograms は log-mel に DCT(type=2, norm='ortho') をかけて先頭NUM_MFCCを取る\n",
    "dct_full = tf.signal.dct(tf.eye(NUM_MEL_BINS), type=2, norm='ortho')  # [M, M]\n",
    "dct_m = dct_full[:, :NUM_MFCC]                                        # [M, NUM_MFCC]\n",
    "dct_m_np = dct_m.numpy().astype(\"float32\")\n",
    "\n",
    "with open(export_dir / \"dct_matrix.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"shape\": list(dct_m_np.shape),\n",
    "        \"data\": dct_m_np.flatten().tolist()\n",
    "    }, f)\n",
    "\n",
    "print(\"Saved feature matrices:\",\n",
    "      (export_dir / \"mel_weight_matrix.json\").name,\n",
    "      (export_dir / \"dct_matrix.json\").name)\n",
    "\n",
    "# zip 化してダウンロード\n",
    "zip_path = WORK / \"tfjs_model_tfonly.zip\"\n",
    "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "    for p in export_dir.iterdir():\n",
    "        z.write(p, arcname=p.name)\n",
    "\n",
    "print(\"ZIP:\", zip_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81143ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 10-B) 生成物をダウンロード\n",
    "from google.colab import files\n",
    "files.download(str(WORK / \"tfjs_model_tfonly.zip\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c357db",
   "metadata": {},
   "source": [
    "## 11) （参考）ブラウザ側で使うときの最低限の考え方\n",
    "- `model.json` をホスティングして `tf.loadLayersModel()` で読む\n",
    "- マイク入力を **16kHz / 1秒** に揃える\n",
    "- Python 側と同じ前処理（STFT→mel→log→MFCC）を TFJS でも行う\n",
    "\n",
    "`speech-commands` ライブラリは、Speech Commands 系の音声認識をブラウザで扱うためのモジュールで、既定語彙に `unknown` と `background_noise` を含みます citeturn7search5turn7search7  \n",
    "ただし、このノートのモデルは **MFCC を自前計算**する前提なので、JS側でも同じ特徴量生成を実装して推論してください（実装例のセルは必要なら次で追加できます）。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}